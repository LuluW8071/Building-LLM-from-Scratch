{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bi-Gram Lanaguage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™']\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "with open(\"../samples/wizard_of_oz.txt\", \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31, 42, 45, 42, 47, 35, 52,  1, 28, 41, 31,  1, 47, 35, 32,  1, 50, 36,\n",
      "        53, 28, 45, 31,  1, 36, 41,  1, 42, 53,  0,  0,  0,  1,  1, 28,  1, 33,\n",
      "        28, 36, 47, 35, 33, 48, 39,  1, 45, 32, 30, 42, 45, 31])\n"
     ]
    }
   ],
   "source": [
    "string_to_int = {char: i for i, char in enumerate(chars)}\n",
    "int_to_string = {i: char for i, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join(int_to_string[i] for i in l)\n",
    "\n",
    "# Encoding entire text within wizard_of_oz.txt\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample_size: 198485 49622\n",
      "Inputs:\n",
      " tensor([[42, 48, 46, 39, 52,  1, 30, 42],\n",
      "        [52, 32, 28, 45, 46, 11,  1, 29],\n",
      "        [ 1, 46, 35, 32,  1, 50, 28, 46],\n",
      "        [52, 42, 48, 41, 34,  1, 34, 36],\n",
      "        [ 1, 32, 28, 34, 32, 45, 39, 52],\n",
      "        [47, 35, 32,  1, 32, 41, 32, 40],\n",
      "        [32,  1, 47, 42, 43,  1, 42, 33],\n",
      "        [ 1, 52, 32, 39, 39, 42, 50,  1],\n",
      "        [32, 31,  1, 37, 36, 40, 11,  1],\n",
      "        [32, 30, 32,  1, 42, 33,  1, 46],\n",
      "        [45, 46,  1, 50, 36, 39, 39,  1],\n",
      "        [ 1, 36,  1, 33, 42, 48, 41, 31],\n",
      "        [47,  1, 36, 46,  1, 47, 35, 32],\n",
      "        [13,  1,  1, 28, 39, 46, 42, 11],\n",
      "        [47,  1, 36, 41, 47, 42,  0, 28],\n",
      "        [47, 28, 47, 36, 42, 41, 11,  1],\n",
      "        [36, 41, 46, 27,  3,  1, 28, 46],\n",
      "        [30, 35,  1, 40, 52,  1, 47, 35],\n",
      "        [ 1, 28, 34, 28, 36, 41,  1, 28],\n",
      "        [35, 32, 45,  0, 28, 41, 31,  1],\n",
      "        [32, 51, 43, 32, 30, 47, 32, 31],\n",
      "        [33,  1, 50, 42, 42, 31, 11,  1],\n",
      "        [ 0,  0,  3, 47, 45, 48, 32, 11],\n",
      "        [ 1, 28, 41, 31,  1, 28,  1, 40],\n",
      "        [45, 32, 26,  1,  1, 46, 42,  1],\n",
      "        [ 1, 50, 32, 36, 34, 35, 47,  1],\n",
      "        [32, 45, 11,  1, 35, 42, 50, 32],\n",
      "        [13,  3,  0,  0,  3, 50, 32,  1],\n",
      "        [32,  1, 36, 41, 47, 42,  1, 28],\n",
      "        [47, 32, 40, 43, 47,  1, 40, 32],\n",
      "        [32,  1, 47, 42,  1, 47, 35, 32],\n",
      "        [35, 32,  1, 43, 45, 36, 41, 30]])\n",
      "Targets:\n",
      " tensor([[48, 46, 39, 52,  1, 30, 42, 39],\n",
      "        [32, 28, 45, 46, 11,  1, 29, 48],\n",
      "        [46, 35, 32,  1, 50, 28, 46,  1],\n",
      "        [42, 48, 41, 34,  1, 34, 36, 45],\n",
      "        [32, 28, 34, 32, 45, 39, 52, 13],\n",
      "        [35, 32,  1, 32, 41, 32, 40, 52],\n",
      "        [ 1, 47, 42, 43,  1, 42, 33,  1],\n",
      "        [52, 32, 39, 39, 42, 50,  1, 35],\n",
      "        [31,  1, 37, 36, 40, 11,  1, 50],\n",
      "        [30, 32,  1, 42, 33,  1, 46, 47],\n",
      "        [46,  1, 50, 36, 39, 39,  1, 34],\n",
      "        [36,  1, 33, 42, 48, 41, 31,  1],\n",
      "        [ 1, 36, 46,  1, 47, 35, 32,  1],\n",
      "        [ 1,  1, 28, 39, 46, 42, 11,  1],\n",
      "        [ 1, 36, 41, 47, 42,  0, 28, 41],\n",
      "        [28, 47, 36, 42, 41, 11,  1, 29],\n",
      "        [41, 46, 27,  3,  1, 28, 46, 38],\n",
      "        [35,  1, 40, 52,  1, 47, 35, 45],\n",
      "        [28, 34, 28, 36, 41,  1, 28, 41],\n",
      "        [32, 45,  0, 28, 41, 31,  1, 34],\n",
      "        [51, 43, 32, 30, 47, 32, 31, 39],\n",
      "        [ 1, 50, 42, 42, 31, 11,  1, 28],\n",
      "        [ 0,  3, 47, 45, 48, 32, 11,  3],\n",
      "        [28, 41, 31,  1, 28,  1, 40, 42],\n",
      "        [32, 26,  1,  1, 46, 42,  1, 28],\n",
      "        [50, 32, 36, 34, 35, 47,  1, 42],\n",
      "        [45, 11,  1, 35, 42, 50, 32, 49],\n",
      "        [ 3,  0,  0,  3, 50, 32,  1, 42],\n",
      "        [ 1, 36, 41, 47, 42,  1, 28,  1],\n",
      "        [32, 40, 43, 47,  1, 40, 32,  1],\n",
      "        [ 1, 47, 42,  1, 47, 35, 32,  1],\n",
      "        [32,  1, 43, 45, 36, 41, 30, 32]])\n",
      "torch.Size([32, 8]) torch.Size([32, 8])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 32\n",
    "\n",
    "# Split 80:20 for train and val\n",
    "n = int(0.8 * len(data))\n",
    "\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "print(\"Sample_size:\", len(train_data), len(val_data))\n",
    "\n",
    "def get_batch(split: str):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "\n",
    "    # Generates batch_size random starting indices (ix) within the range [0, len(data) - block_size).\n",
    "    # Ensures that each index i can extract a full sequence of length block_size without exceeding the dataset length.\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # print(ix)\n",
    "\n",
    "    # Generate Input and Target Batch (character level token)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])         # Input\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])     # Targets: Input Shift right by 1\n",
    "\n",
    "    # Push batches to device (preferrebly to CUDA)\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "print(\"Inputs:\\n\", x)\n",
    "print(\"Targets:\\n\", y)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-Gram Language Model\n",
    "class BigramLangaugeModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets = None):\n",
    "        logits = self.token_embedding(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # Unpack logits shape to batch, seq_len, class\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # Reshape 3D logits -> 2D logits\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            # Compute loss fn\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(index)\n",
    "\n",
    "            # Take only last time step\n",
    "            logits = logits[:, -1, :]   # (B, C)\n",
    "\n",
    "            # Apply softmax to get probs\n",
    "            probs = F.softmax(logits, dim=-1)   \n",
    "\n",
    "            # Sample from distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)     # (B, 1)\n",
    "\n",
    "            # Append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1)   # (B, T+1)\n",
    "\n",
    "        return index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLangaugeModel(\n",
       "  (token_embedding): Embedding(61, 61)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BigramLangaugeModel(vocab_size).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ,\n",
      "“'&’2'$5’ ™e''/&.3o*$yyy4.6•0n:8&—21:uxd6th!$?tfz9\n",
      "8ty%fc',h.k*/,w.*$’gl1r(m37g8:,”9“bv%bg7%’”n'mw”%udc—1“o*9\n",
      "dc—cc$'z4n3,h(!\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device =device)\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=128)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 0 | Train Loss: 4.570227146148682 | Validation Loss: 4.58008337020874\n",
      "Steps: 25000 | Train Loss: 4.0866804122924805 | Validation Loss: 3.995805263519287\n",
      "Steps: 50000 | Train Loss: 3.6749634742736816 | Validation Loss: 3.5476741790771484\n",
      "Steps: 75000 | Train Loss: 3.1945066452026367 | Validation Loss: 3.2438645362854004\n",
      "Steps: 100000 | Train Loss: 2.948679208755493 | Validation Loss: 3.0662708282470703\n",
      "Steps: 125000 | Train Loss: 2.778599739074707 | Validation Loss: 2.869990587234497\n",
      "Steps: 150000 | Train Loss: 2.645695209503174 | Validation Loss: 2.6486706733703613\n",
      "Steps: 175000 | Train Loss: 2.512050151824951 | Validation Loss: 2.43312668800354\n",
      "Steps: 200000 | Train Loss: 2.5036418437957764 | Validation Loss: 2.5253477096557617\n",
      "Steps: 225000 | Train Loss: 2.368298053741455 | Validation Loss: 2.7297329902648926\n",
      "Steps: 250000 | Train Loss: 2.362368106842041 | Validation Loss: 2.496636390686035\n",
      "Steps: 275000 | Train Loss: 2.555128335952759 | Validation Loss: 2.534484386444092\n",
      "Steps: 300000 | Train Loss: 2.343989133834839 | Validation Loss: 2.524866819381714\n",
      "Steps: 325000 | Train Loss: 2.319908857345581 | Validation Loss: 2.521049737930298\n",
      "Steps: 350000 | Train Loss: 2.4994606971740723 | Validation Loss: 2.5739591121673584\n",
      "Steps: 375000 | Train Loss: 2.3300089836120605 | Validation Loss: 2.514204502105713\n",
      "Steps: 400000 | Train Loss: 2.3156888484954834 | Validation Loss: 2.598306894302368\n",
      "Steps: 425000 | Train Loss: 2.3163466453552246 | Validation Loss: 2.5700135231018066\n",
      "Steps: 450000 | Train Loss: 2.3372247219085693 | Validation Loss: 2.5548465251922607\n",
      "Steps: 475000 | Train Loss: 2.4155099391937256 | Validation Loss: 2.468566656112671\n",
      "Steps: 500000 | Train Loss: 2.331228494644165 | Validation Loss: 2.6515867710113525\n",
      "Steps: 525000 | Train Loss: 2.2986221313476562 | Validation Loss: 2.5307788848876953\n",
      "Steps: 550000 | Train Loss: 2.37276291847229 | Validation Loss: 2.6078250408172607\n",
      "Steps: 575000 | Train Loss: 2.400995969772339 | Validation Loss: 2.6523609161376953\n",
      "Steps: 600000 | Train Loss: 2.3302178382873535 | Validation Loss: 2.686929702758789\n",
      "Steps: 625000 | Train Loss: 2.4658584594726562 | Validation Loss: 2.621152877807617\n",
      "Steps: 650000 | Train Loss: 2.280426025390625 | Validation Loss: 2.442478656768799\n",
      "Steps: 675000 | Train Loss: 2.4033353328704834 | Validation Loss: 2.6418344974517822\n",
      "Steps: 700000 | Train Loss: 2.4298458099365234 | Validation Loss: 2.488793134689331\n",
      "Steps: 725000 | Train Loss: 2.485952377319336 | Validation Loss: 2.654075860977173\n",
      "Steps: 750000 | Train Loss: 2.357879877090454 | Validation Loss: 2.3016436100006104\n",
      "Steps: 775000 | Train Loss: 2.33711576461792 | Validation Loss: 2.6745731830596924\n",
      "Steps: 800000 | Train Loss: 2.321446418762207 | Validation Loss: 2.6723570823669434\n",
      "Steps: 825000 | Train Loss: 2.4053797721862793 | Validation Loss: 2.512103319168091\n",
      "Steps: 850000 | Train Loss: 2.3997459411621094 | Validation Loss: 2.510584831237793\n",
      "Steps: 875000 | Train Loss: 2.3499460220336914 | Validation Loss: 2.5770678520202637\n",
      "Steps: 900000 | Train Loss: 2.578115940093994 | Validation Loss: 2.9915285110473633\n",
      "Steps: 925000 | Train Loss: 2.39361572265625 | Validation Loss: 2.4832258224487305\n",
      "Steps: 950000 | Train Loss: 2.38392972946167 | Validation Loss: 2.741558313369751\n",
      "Steps: 975000 | Train Loss: 2.469313859939575 | Validation Loss: 2.6208786964416504\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "max_iters = 1_000_000\n",
    "eval_interval = 25_000 \n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # Training step\n",
    "    model.train()\n",
    "    x_train, y_train = get_batch(\"train\")\n",
    "    logits, loss = model.forward(x_train, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    if iter % eval_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_val, y_val = get_batch(\"val\")\n",
    "            val_logits, val_loss = model.forward(x_val, y_val)\n",
    "            print(f\"Steps: {iter} | Train Loss: {loss.item()} | Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]])\n",
      "\n",
      "\n",
      "\n",
      "\"laroskesistofrmplares whacopr\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(context)\n",
    "\n",
    "generated_chars = decode(model.generate(context, max_new_tokens=32)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there friend co were hyohinl\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello there friend\"\n",
    "sample_context = torch.tensor(encode(sample_text.lower()), dtype=torch.long, device=device).unsqueeze(0)\n",
    "generated_chars = decode(model.generate(sample_context, max_new_tokens=16)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
