{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OupGFDmme-Co"
      },
      "source": [
        "## Building GPT Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4hadacoe-Cr",
        "outputId": "b5265fb9-0ee0-49b1-82c1-f17fecd1e5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQM-b16e-Ct",
        "outputId": "8927bc2a-5ea1-4a11-97a1-b612da9591e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™']\n",
            "87\n"
          ]
        }
      ],
      "source": [
        "chars = \"\"\n",
        "with open(\"../samples/wizard_of_oz.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    chars = sorted(list(set(text)))\n",
        "\n",
        "print(chars)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Ovd2eNfjE2",
        "outputId": "dda34aff-7a52-4601-ccb3-a6ce1fce1d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "They cry Oz--Oz! more about Oz, Mr. Baum! and what can I do but obey their commands?\n",
            "\n",
            "This is Our Book--mine and the children's. For they have flooded me with thousands of suggestions in regard to it, and I have honestly tried to adopt as many of these suggestions as could be fitted into one story.\n",
            "\n",
            "After the wonderful success of Ozma of Oz it is evident that Dorothy has become a firm fixture in these Oz stories. The little ones all love Dorothy, and as one of my small friends aptly states It isn't a real Oz story without her. So here she is again, as sweet and gentle and innocent as ever, I hope, and the heroine of another strange adventure.\n",
            "\n",
            "There were many requests from my little correspondents for more about the Wizard. It seems the jolly old fellow made hosts of friends in the first Oz book, in spite of the fact that he frankly acknowledged himself a humbug. The children had heard how he mounted into the sky in a balloon and they were all waiting for him to come down again. So what could I do but tell what happened to the Wizard afterward? You will find him in these pages, just the same humbug Wizard as before.\n",
            "\n",
            "There was one thing the children demanded which I found it impossible to do in this present book they bade me introduce Toto, Dorothy's little black dog, who has many friends among my readers. But you will see, when you begin to read the story, that Toto was in Kansas while Dorothy was in California, and so she had to start on her adventure without \n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "# Remove unwanted symbols (keep only letters, punctuation, some symbols and spaces)\n",
        "text = re.sub(r'[^a-zA-Z\\s.,;!?()\\'\\-—]', '', text)\n",
        "\n",
        "# Remove single newlines within paragraphs (replace them with space)\n",
        "text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
        "\n",
        "# Ensure paragraph breaks (keep double newlines as they separate paragraphs)\n",
        "text = re.sub(r'\\n{2,}', '\\n\\n', text)  # Replace 3+ newlines with 2\n",
        "text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "print(text[514:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OEUDZAk5UV",
        "outputId": "cecbff96-666c-4bd7-9cca-1cc86cfe5ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—']\n",
            "64\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(chars)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGOEyoZ6e-Cu",
        "outputId": "73913715-be3c-42e5-d690-f7c29a321cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([14, 51, 54, 51, 56, 44, 61,  1, 37, 50, 40,  1, 56, 44, 41,  1, 33, 45,\n",
            "        62, 37, 54, 40,  1, 45, 50,  1, 25, 62,  0,  0,  1, 11,  1, 16, 37, 45,\n",
            "        56, 44, 42, 57, 48,  1, 28, 41, 39, 51, 54, 40,  1, 51])\n"
          ]
        }
      ],
      "source": [
        "string_to_int = {char: i for i, char in enumerate(chars)}\n",
        "int_to_string = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join(int_to_string[i] for i in l)\n",
        "\n",
        "# Encoding entire text within wizard_of_oz.txt\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "n_embed = 384\n",
        "n_layer = 4\n",
        "n_head = 3\n",
        "dropout = 0.2\n",
        "learning_rate = 2e-4 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VERs1zzve-Cv",
        "outputId": "90167846-9ca0-4260-ab68-3db74d5113bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample_size: 194961 48741\n"
          ]
        }
      ],
      "source": [
        "# Split 80:20 for train and val\n",
        "n = int(0.8 * len(data))\n",
        "\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "print(\"Sample_size:\", len(train_data), len(val_data))\n",
        "\n",
        "def get_batch(data, batch_size, block_size):\n",
        "    \"\"\" Batch Generator \"\"\"\n",
        "    num_batches = len(data) // batch_size   # Ensure complete batches\n",
        "    indices = list(range(num_batches * batch_size))\n",
        "    random.shuffle(indices)                 # Shuffle for randomness\n",
        "\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "        batch_indices = indices[i:i + batch_size]\n",
        "        batch_indices = [idx % (len(data) - block_size) for idx in batch_indices]\n",
        "\n",
        "        # Generate Input and Target Batch (character level token)\n",
        "        x = torch.stack([data[i:i + block_size] for i in batch_indices])            # Input\n",
        "        y = torch.stack([data[i + 1:i + block_size + 1] for i in batch_indices])    # Targets: Input Shift right by 1\n",
        "\n",
        "        # Push batches to device (preferrebly to CUDA)\n",
        "        yield x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT Model (Decoder Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Linear Layers follwed by non-linearity \"\"\"\n",
        "    def __init__(self, n_embed, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        ) \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scaled Dot-Product Attention**\n",
        "\n",
        "$$w = \\frac{q \\cdot k^T}{\\sqrt{d_k}}$$\n",
        "\n",
        "where:  \n",
        "- \\( w \\) is the attention score matrix.  \n",
        "- \\( q \\) (query) and \\( k \\) (key) are transformed input embeddings.  \n",
        "- \\( d_k \\) is the **dimensionality of the key vectors** (i.e., `head_size`).  \n",
        "- The scaling factor **$\\frac{1}{\\sqrt{d_k}}$** helps control the magnitude of the dot product.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" Head for Self-Attention (Scaled-Dot Product) \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "\n",
        "        # NOTE: Register a lower triangular matrix as a buffer (used for masking future tokens in self-attention)\n",
        "        # It’s non-trainable, included in state_dict(), and avoids recomputation. \n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (B, T, C) -> Output: (B, T, head_size)\n",
        "        B, T, C = x.shape                   # Unpack Input Dimensions\n",
        "        k, q = self.key(x), self.query(x)   # (B, T, head_size)\n",
        "\n",
        "        # Compute attn_scores(attn_weights) [creating q @ transposed k grid matrix]\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5             # (B, T, head_size) @ # (B, head_size, T) -> (B, T, T)\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))   # (B, T, T)\n",
        "        w = F.softmax(w, dim=-1)                                    # (B, T, T)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Perform the weighted aggregation of the values\n",
        "        v = self.value(x)                   # (B, T, head_size)\n",
        "        out = w @ v                         # (B, T, T) @ # (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "        \n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of Attn in Parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])     # Create heads in parallel\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, T, C) -> Concat feature(last_dim): (B, T, [h0_1, h0_2, h0_3, h0_4, h1_1, h1_2, h1_3, h1_4, h2_1, h2_2, h2_3, h2_4])\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Blocks \"\"\"\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head   # Head_size to capture features\n",
        "        self.self_attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.feed_forward = FeedForward(n_embed)\n",
        "        self.lnorm1 = nn.LayerNorm(n_embed)\n",
        "        self.lnorm2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.self_attn(x)\n",
        "        x = self.lnorm1(x+y)\n",
        "        y = self.feed_forward(x)\n",
        "        x = self.lnorm2(x+y)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYREnhkee-Cw"
      },
      "outputs": [],
      "source": [
        "# GPT Model\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, n_embed, n_head, n_layer):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, n_embed)\n",
        "        self.positional_embeddings = nn.Embedding(block_size, n_embed)\n",
        "\n",
        "        self.decoder_blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        self.final_layer = nn.Linear(n_embed, vocab_size)\n",
        "        self.final_layernorm = nn.LayerNorm(n_embed)\n",
        "\n",
        "        self.apply(self.__init_weights)\n",
        "\n",
        "    \n",
        "    def __init_weights(self, module):\n",
        "        \"\"\" \n",
        "        Initialize proper (gaussian distribution) weights for stable training and convergence\n",
        "        Docs: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.normal_\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Initializes weights with a normal (Gaussian) distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                # Set the biases to zero\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            # Initializes embeddings weights with a normal (Gaussian) distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, index, targets = None):\n",
        "        B, T = index.shape\n",
        "        # Index and targets are both (B, T) tokens of integers\n",
        "        token_embed = self.token_embeddings(index)\n",
        "\n",
        "        # torch.arange(T) -> list of indices\n",
        "        pos_embed = self.positional_embeddings(torch.arange(T, device=device))   # (T, C)\n",
        "        x = token_embed + pos_embed     # (B, T, C)\n",
        "        x = self.decoder_blocks(x)      # (B, T, C)\n",
        "        x = self.final_layernorm(x)     # (B, T, C)\n",
        "        logits = self.final_layer(x)    # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Unpack logits shape to batch, seq_len, class\n",
        "            B, T, C = logits.shape\n",
        "            # Reshape 3D logits -> 2D logits\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            # Compute loss fn\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self.forward(index)\n",
        "\n",
        "            # Take only last time step\n",
        "            logits = logits[:, -1, :]   # (B, C)\n",
        "\n",
        "            # Apply softmax to get probs\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1)     # (B, 1)\n",
        "\n",
        "            # Append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1)   # (B, T+1)\n",
        "\n",
        "        return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXO0BNpDe-Cx",
        "outputId": "1eb63cf8-83a5-474e-a956-616ef15a2333"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (token_embeddings): Embedding(64, 384)\n",
              "  (positional_embeddings): Embedding(8, 384)\n",
              "  (decoder_blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Linear(in_features=384, out_features=64, bias=True)\n",
              "  (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPTModel(vocab_size, block_size, n_embed, n_head, n_layer).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpWh1wp9e-Cx",
        "outputId": "c70b1ddd-fd93-4d26-811a-7617bbc30211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hxm?lsDItPQp!-.s(S)l;XXU—hg!fXeV\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=32)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "djjx5BbXe-Cy"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)  # Decay LR\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvpjqYKTe-Cy",
        "outputId": "45a4231a-da9a-490c-8cd8-01d9a0dffa6c"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[93], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 11\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     14\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:137\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    136\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adamw.py:220\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    210\u001b[0m         group,\n\u001b[1;32m    211\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m         state_steps,\n\u001b[1;32m    218\u001b[0m     )\n\u001b[0;32m--> 220\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adamw.py:782\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    780\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 782\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/optim/adamw.py:427\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    425\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for x_train, y_train in get_batch(train_data, batch_size, block_size):\n",
        "        logits, loss = model(x_train, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_batches\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in get_batch(val_data, batch_size, block_size):\n",
        "            _, val_loss = model(x_val, y_val)\n",
        "            total_val_loss += val_loss.item()\n",
        "            num_val_batches += 1\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_val_batches\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyCxSXsYe-Cz",
        "outputId": "48f3b7ea-8ddd-4f50-85ea-74b4d4ccb581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0]], device='cuda:0')\n",
            "\n",
            "Thes fo ssad he, thtond tazm s, \n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(context)\n",
        "\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=32)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCf7UG_re-C0",
        "outputId": "114e74de-b259-4e7f-96ef-21358a9e59b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wizard of oz at beand\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"wizard of oz \"\n",
        "sample_context = torch.tensor(encode(sample_text), dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated_chars = decode(model.generate(sample_context, max_new_tokens=8)[0].tolist())\n",
        "print(generated_chars)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
