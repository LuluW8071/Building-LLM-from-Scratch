{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OupGFDmme-Co"
      },
      "source": [
        "## Building GPT Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4hadacoe-Cr",
        "outputId": "a5b2b37b-694b-48bf-81e3-8595c9e45ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQM-b16e-Ct",
        "outputId": "55e0033f-8cbd-49d3-e569-33031d6be29c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™']\n",
            "87\n"
          ]
        }
      ],
      "source": [
        "chars = \"\"\n",
        "with open(\"wizard_of_oz.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    chars = sorted(list(set(text)))\n",
        "\n",
        "print(chars)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Ovd2eNfjE2",
        "outputId": "6a8b3d79-cb28-4a75-d22c-3827773ce74a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "They cry Oz--Oz! more about Oz, Mr. Baum! and what can I do but obey their commands?\n",
            "\n",
            "This is Our Book--mine and the children's. For they have flooded me with thousands of suggestions in regard to it, and I have honestly tried to adopt as many of these suggestions as could be fitted into one story.\n",
            "\n",
            "After the wonderful success of Ozma of Oz it is evident that Dorothy has become a firm fixture in these Oz stories. The little ones all love Dorothy, and as one of my small friends aptly states It isn't a real Oz story without her. So here she is again, as sweet and gentle and innocent as ever, I hope, and the heroine of another strange adventure.\n",
            "\n",
            "There were many requests from my little correspondents for more about the Wizard. It seems the jolly old fellow made hosts of friends in the first Oz book, in spite of the fact that he frankly acknowledged himself a humbug. The children had heard how he mounted into the sky in a balloon and they were all waiting for him to come down again. So what could I do but tell what happened to the Wizard afterward? You will find him in these pages, just the same humbug Wizard as before.\n",
            "\n",
            "There was one thing the children demanded which I found it impossible to do in this present book they bade me introduce Toto, Dorothy's little black dog, who has many friends among my readers. But you will see, when you begin to read the story, that Toto was in Kansas while Dorothy was in California, and so she had to start on her adventure without \n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "# Remove unwanted symbols (keep only letters, punctuation, some symbols and spaces)\n",
        "text = re.sub(r'[^a-zA-Z\\s.,;!?()\\'\\-—]', '', text)\n",
        "\n",
        "# Remove single newlines within paragraphs (replace them with space)\n",
        "text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
        "\n",
        "# Ensure paragraph breaks (keep double newlines as they separate paragraphs)\n",
        "text = re.sub(r'\\n{2,}', '\\n\\n', text)  # Replace 3+ newlines with 2\n",
        "text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "print(text[514:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OEUDZAk5UV",
        "outputId": "f6e038cf-a19c-4268-bc65-c8d380a129bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', \"'\", '(', ')', ',', '-', '.', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—']\n",
            "64\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "print(chars)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGOEyoZ6e-Cu",
        "outputId": "3de65aa3-93f5-4cb6-bd48-45a817961a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([14, 51, 54, 51, 56, 44, 61,  1, 37, 50, 40,  1, 56, 44, 41,  1, 33, 45,\n",
            "        62, 37, 54, 40,  1, 45, 50,  1, 25, 62,  0,  0,  1, 11,  1, 16, 37, 45,\n",
            "        56, 44, 42, 57, 48,  1, 28, 41, 39, 51, 54, 40,  1, 51])\n"
          ]
        }
      ],
      "source": [
        "string_to_int = {char: i for i, char in enumerate(chars)}\n",
        "int_to_string = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join(int_to_string[i] for i in l)\n",
        "\n",
        "# Encoding entire text within wizard_of_oz.txt\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cj43CoQVEO3u"
      },
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "block_size = 128\n",
        "batch_size = 128\n",
        "\n",
        "n_embed = 384\n",
        "n_layer = 4\n",
        "n_head = 3\n",
        "dropout = 0.2\n",
        "learning_rate = 3e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VERs1zzve-Cv",
        "outputId": "86080335-1c86-4ef9-b6d1-c44fbaa34af9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample_size: 194961 48741\n"
          ]
        }
      ],
      "source": [
        "# Split 80:20 for train and val\n",
        "n = int(0.8 * len(data))\n",
        "\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "print(\"Sample_size:\", len(train_data), len(val_data))\n",
        "\n",
        "def get_batch(data, batch_size, block_size):\n",
        "    \"\"\" Batch Generator \"\"\"\n",
        "    num_batches = len(data) // batch_size   # Ensure complete batches\n",
        "    indices = list(range(num_batches * batch_size))\n",
        "    random.shuffle(indices)                 # Shuffle for randomness\n",
        "\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "        batch_indices = indices[i:i + batch_size]\n",
        "        batch_indices = [idx % (len(data) - block_size) for idx in batch_indices]\n",
        "\n",
        "        # Generate Input and Target Batch (character level token)\n",
        "        x = torch.stack([data[i:i + block_size] for i in batch_indices])            # Input\n",
        "        y = torch.stack([data[i + 1:i + block_size + 1] for i in batch_indices])    # Targets: Input Shift right by 1\n",
        "\n",
        "        # Push batches to device (preferrebly to CUDA)\n",
        "        yield x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw50lilZEO3v"
      },
      "source": [
        "### GPT Model (Decoder Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yzLu8gQjEO3v"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Linear Layers follwed by non-linearity \"\"\"\n",
        "    def __init__(self, n_embed, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HztZweQEO3w"
      },
      "source": [
        "**Scaled Dot-Product Attention**\n",
        "\n",
        "$$w = \\frac{q \\cdot k^T}{\\sqrt{d_k}}$$\n",
        "\n",
        "where:  \n",
        "- \\( w \\) is the attention score matrix.  \n",
        "- \\( q \\) (query) and \\( k \\) (key) are transformed input embeddings.  \n",
        "- \\( d_k \\) is the **dimensionality of the key vectors** (i.e., `head_size`).  \n",
        "- The scaling factor **$\\frac{1}{\\sqrt{d_k}}$** helps control the magnitude of the dot product.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ab18oiSNEO3x"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" Head for Self-Attention (Scaled-Dot Product) \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "\n",
        "        # NOTE: Register a lower triangular matrix as a buffer (used for masking future tokens in self-attention)\n",
        "        # It’s non-trainable, included in state_dict(), and avoids recomputation.\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (B, T, C) -> Output: (B, T, head_size)\n",
        "        B, T, C = x.shape                   # Unpack Input Dimensions\n",
        "        k, q = self.key(x), self.query(x)   # (B, T, head_size)\n",
        "\n",
        "        # Compute attn_scores(attn_weights) [creating q @ transposed k grid matrix]\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5             # (B, T, head_size) @ # (B, head_size, T) -> (B, T, T)\n",
        "\n",
        "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))   # (B, T, T)\n",
        "        w = F.softmax(w, dim=-1)                                    # (B, T, T)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Perform the weighted aggregation of the values\n",
        "        v = self.value(x)                   # (B, T, head_size)\n",
        "        out = w @ v                         # (B, T, T) @ # (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of Attn in Parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])     # Create heads in parallel\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, T, C) -> Concat feature(last_dim): (B, T, [h0_1, h0_2, h0_3, h0_4, h1_1, h1_2, h1_3, h1_4, h2_1, h2_2, h2_3, h2_4])\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XrlPHJrIEO3x"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Blocks \"\"\"\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head   # Head_size to capture features\n",
        "        self.self_attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.feed_forward = FeedForward(n_embed)\n",
        "        self.lnorm1 = nn.LayerNorm(n_embed)\n",
        "        self.lnorm2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.self_attn(x)\n",
        "        x = self.lnorm1(x+y)\n",
        "        y = self.feed_forward(x)\n",
        "        x = self.lnorm2(x+y)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OYREnhkee-Cw"
      },
      "outputs": [],
      "source": [
        "# GPT Model\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, n_embed, n_head, n_layer):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, n_embed)\n",
        "        self.positional_embeddings = nn.Embedding(block_size, n_embed)\n",
        "\n",
        "        self.decoder_blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        self.final_layer = nn.Linear(n_embed, vocab_size)\n",
        "        self.final_layernorm = nn.LayerNorm(n_embed)\n",
        "\n",
        "        self.apply(self.__init_weights)\n",
        "\n",
        "\n",
        "    def __init_weights(self, module):\n",
        "        \"\"\"\n",
        "        Initialize proper (gaussian distribution) weights for stable training and convergence\n",
        "        Docs: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.normal_\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Initializes weights with a normal (Gaussian) distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                # Set the biases to zero\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            # Initializes embeddings weights with a normal (Gaussian) distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, index, targets = None):\n",
        "        B, T = index.shape\n",
        "        # Index and targets are both (B, T) tokens of integers\n",
        "        token_embed = self.token_embeddings(index)\n",
        "\n",
        "        # torch.arange(T) -> list of indices\n",
        "        pos_embed = self.positional_embeddings(torch.arange(T, device=device))   # (T, C)\n",
        "        x = token_embed + pos_embed     # (B, T, C)\n",
        "        x = self.decoder_blocks(x)      # (B, T, C)\n",
        "        x = self.final_layernorm(x)     # (B, T, C)\n",
        "        logits = self.final_layer(x)    # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Unpack logits shape to batch, seq_len, class\n",
        "            B, T, C = logits.shape\n",
        "            # Reshape 3D logits -> 2D logits\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            # Compute loss fn\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self.forward(index)\n",
        "\n",
        "            # Take only last time step\n",
        "            logits = logits[:, -1, :]   # (B, C)\n",
        "\n",
        "            # Apply softmax to get probs\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1)     # (B, 1)\n",
        "\n",
        "            # Append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1)   # (B, T+1)\n",
        "\n",
        "        return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXO0BNpDe-Cx",
        "outputId": "71920f38-0afa-4844-b588-706c0f264c1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (token_embeddings): Embedding(64, 384)\n",
              "  (positional_embeddings): Embedding(128, 384)\n",
              "  (decoder_blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-2): 3 x Head(\n",
              "            (key): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=128, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (linear_layers): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (lnorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (lnorm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (final_layer): Linear(in_features=384, out_features=64, bias=True)\n",
              "  (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPTModel(vocab_size, block_size, n_embed, n_head, n_layer).to(device)\n",
        "# model = torch.compile(model)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpWh1wp9e-Cx",
        "outputId": "3adb21df-e6f5-431e-d278-3c70ba81f75f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ".vQDKrwHxpzLzmQcv'xTu'!\n",
            "be'gL'A(\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=32)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyCxSXsYe-Cz",
        "outputId": "76c581c9-b880-42b1-9b14-5473e0bbaf97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0]], device='cuda:0')\n",
            "\n",
            "y?IbarBiFhIewVZiiWbpKkGU,UFGicfd\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(context)\n",
        "\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=32)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "djjx5BbXe-Cy"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)  # Decay LR\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvpjqYKTe-Cy",
        "outputId": "cd5dc4fd-794e-44c1-ec66-04323b265f03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Step 0 | Train Loss: 4.2393\n",
            " Step 250 | Train Loss: 2.4694\n",
            " Step 500 | Train Loss: 2.3975\n",
            " Step 750 | Train Loss: 2.3579\n",
            " Step 1000 | Train Loss: 2.3116\n",
            " Step 1250 | Train Loss: 2.2103\n",
            " Step 1500 | Train Loss: 2.1347\n",
            "Epoch 1/10 | Train Loss: 2.3899 | Validation Loss: 2.3234\n",
            " Step 0 | Train Loss: 2.1289\n",
            " Step 250 | Train Loss: 2.0710\n",
            " Step 500 | Train Loss: 1.9918\n",
            " Step 750 | Train Loss: 1.9304\n",
            " Step 1000 | Train Loss: 1.8857\n",
            " Step 1250 | Train Loss: 1.8174\n",
            " Step 1500 | Train Loss: 1.7852\n",
            "Epoch 2/10 | Train Loss: 1.9425 | Validation Loss: 2.0514\n",
            " Step 0 | Train Loss: 1.7820\n",
            " Step 250 | Train Loss: 1.7536\n",
            " Step 500 | Train Loss: 1.7351\n",
            " Step 750 | Train Loss: 1.6955\n",
            " Step 1000 | Train Loss: 1.6842\n",
            " Step 1250 | Train Loss: 1.6371\n",
            " Step 1500 | Train Loss: 1.6209\n",
            "Epoch 3/10 | Train Loss: 1.7015 | Validation Loss: 1.9336\n",
            " Step 0 | Train Loss: 1.5947\n",
            " Step 250 | Train Loss: 1.5917\n",
            " Step 500 | Train Loss: 1.5878\n",
            " Step 750 | Train Loss: 1.5648\n",
            " Step 1000 | Train Loss: 1.5531\n",
            " Step 1250 | Train Loss: 1.5288\n",
            " Step 1500 | Train Loss: 1.5187\n",
            "Epoch 4/10 | Train Loss: 1.5693 | Validation Loss: 1.8644\n",
            " Step 0 | Train Loss: 1.5010\n",
            " Step 250 | Train Loss: 1.5154\n",
            " Step 500 | Train Loss: 1.4959\n",
            " Step 750 | Train Loss: 1.4908\n",
            " Step 1000 | Train Loss: 1.4478\n",
            " Step 1250 | Train Loss: 1.4413\n",
            " Step 1500 | Train Loss: 1.4180\n",
            "Epoch 5/10 | Train Loss: 1.4811 | Validation Loss: 1.8276\n",
            " Step 0 | Train Loss: 1.4219\n",
            " Step 250 | Train Loss: 1.4301\n",
            " Step 500 | Train Loss: 1.4225\n",
            " Step 750 | Train Loss: 1.4110\n",
            " Step 1000 | Train Loss: 1.4059\n",
            " Step 1250 | Train Loss: 1.3757\n",
            " Step 1500 | Train Loss: 1.4004\n",
            "Epoch 6/10 | Train Loss: 1.4193 | Validation Loss: 1.8060\n",
            " Step 0 | Train Loss: 1.3721\n",
            " Step 250 | Train Loss: 1.3868\n",
            " Step 500 | Train Loss: 1.3622\n",
            " Step 750 | Train Loss: 1.3862\n",
            " Step 1000 | Train Loss: 1.3805\n",
            " Step 1250 | Train Loss: 1.3907\n",
            " Step 1500 | Train Loss: 1.3611\n",
            "Epoch 7/10 | Train Loss: 1.3746 | Validation Loss: 1.7947\n",
            " Step 0 | Train Loss: 1.3680\n",
            " Step 250 | Train Loss: 1.3644\n",
            " Step 500 | Train Loss: 1.3588\n",
            " Step 750 | Train Loss: 1.3401\n",
            " Step 1000 | Train Loss: 1.3109\n",
            " Step 1250 | Train Loss: 1.3346\n",
            " Step 1500 | Train Loss: 1.3213\n",
            "Epoch 8/10 | Train Loss: 1.3360 | Validation Loss: 1.7869\n",
            " Step 0 | Train Loss: 1.3059\n",
            " Step 250 | Train Loss: 1.2879\n",
            " Step 500 | Train Loss: 1.3148\n",
            " Step 750 | Train Loss: 1.3122\n",
            " Step 1000 | Train Loss: 1.3076\n",
            " Step 1250 | Train Loss: 1.2837\n",
            " Step 1500 | Train Loss: 1.2917\n",
            "Epoch 9/10 | Train Loss: 1.3017 | Validation Loss: 1.7858\n",
            " Step 0 | Train Loss: 1.2742\n",
            " Step 250 | Train Loss: 1.2646\n",
            " Step 500 | Train Loss: 1.2882\n",
            " Step 750 | Train Loss: 1.2677\n",
            " Step 1000 | Train Loss: 1.2510\n",
            " Step 1250 | Train Loss: 1.2813\n",
            " Step 1500 | Train Loss: 1.2735\n",
            "Epoch 10/10 | Train Loss: 1.2711 | Validation Loss: 1.7859\n",
            "Training Complete!\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for step, (x_train, y_train) in enumerate(get_batch(train_data, batch_size, block_size)):\n",
        "        logits, loss = model(x_train, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "        # Print step-wise progress\n",
        "        if step % 250 == 0:\n",
        "            print(f\" Step {step} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_batches\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in get_batch(val_data, batch_size, block_size):\n",
        "            _, val_loss = model(x_val, y_val)\n",
        "            total_val_loss += val_loss.item()\n",
        "            num_val_batches += 1\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_val_batches\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCf7UG_re-C0",
        "outputId": "e1dbbb0b-1c0e-489f-e7c4-c163c4476ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello wizard his place and enduch and his head eyes, with much down the wood\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"Hello wizard\"\n",
        "sample_context = torch.tensor(encode(sample_text), dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated_chars = decode(model.generate(sample_context, max_new_tokens=64)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3iNxchKoV5c",
        "outputId": "f231fb9d-67a5-4c27-8102-e7d3296d9727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There was one thing the children demanded mindly pigletion of the strange., I am sure in the stand felt d\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"There was one thing the children demanded\"\n",
        "sample_context = torch.tensor(encode(sample_text), dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated_chars = decode(model.generate(sample_context, max_new_tokens=64)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AXCGmRqSRe54"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('wizard_oz_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "PEML-WZfuLMt"
      },
      "outputs": [],
      "source": [
        "def gpt_generation(sample_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates text using a GPT model.\n",
        "\n",
        "    Args:\n",
        "        sample_text (str): The input text to generate from.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    sample_context = torch.tensor(encode(sample_text), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    generated_chars = decode(model.generate(sample_context, max_new_tokens=64)[0].tolist())\n",
        "    return generated_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-PkmVKAuwFW",
        "outputId": "12a4c41c-b9d6-45de-aad9-531ee2034768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What a wonderful day, I wish its stays the same of the roan of Voe. They're hunging came soon do not, spired; b\n"
          ]
        }
      ],
      "source": [
        "generation = gpt_generation(\"What a wonderful day, I wish its stays the same\")\n",
        "print(generation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
