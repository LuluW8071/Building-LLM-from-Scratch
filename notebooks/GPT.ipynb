{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OupGFDmme-Co"
      },
      "source": [
        "## Building GPT Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4hadacoe-Cr",
        "outputId": "b5265fb9-0ee0-49b1-82c1-f17fecd1e5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQM-b16e-Ct",
        "outputId": "8927bc2a-5ea1-4a11-97a1-b612da9591e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—', '‘', '’', '“', '”', '•', '™']\n",
            "87\n"
          ]
        }
      ],
      "source": [
        "with open(\"../samples/wizard_of_oz.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(set(text))\n",
        "print(chars)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3Ovd2eNfjE2",
        "outputId": "dda34aff-7a52-4601-ccb3-a6ce1fce1d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "They cry Oz--Oz! more about Oz, Mr. Baum! and what can I do but obey their commands?\n",
            "\n",
            "This is Our Book--mine and the children's. For they have flooded me with thousands of suggestions in regard to it, and I have honestly tried to adopt as many of these suggestions as could be fitted into one story.\n",
            "\n",
            "After the wonderful success of Ozma of Oz it is evident that Dorothy has become a firm fixture in these Oz stories. The little ones all love Dorothy, and as one of my small friends aptly states It isn't a real Oz story without her. So here she is again, as sweet and gentle and innocent as ever, I hope, and the heroine of another strange adventure.\n",
            "\n",
            "There were many requests from my little correspondents for more about the Wizard. It seems the jolly old fellow made hosts of friends in the first Oz book, in spite of the fact that he frankly acknowledged himself a humbug. The children had heard how he mounted into the sky in a balloon and they were all waiting for him to come down again. So what could I do but tell what happened to the Wizard afterward? You will find him in these pages, just the same humbug Wizard as before.\n",
            "\n",
            "There was one thing the children demanded which I found it impossible to do in this present book they bade me introduce Toto, Dorothy's little black dog, who has many friends among my readers. But you will see, when you begin to read the story, that Toto was in Kansas while Dorothy was in California, and so she had to start on her adventure without \n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import re\n",
        "\n",
        "# Remove unwanted symbols (keep only letters, punctuation, some symbols and spaces)\n",
        "text = re.sub(r'[^a-zA-Z\\s.,;!?\\'\\-—]', '', text)\n",
        "\n",
        "# Remove single newlines within paragraphs (replace them with space)\n",
        "text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)\n",
        "\n",
        "# Ensure paragraph breaks (keep double newlines as they separate paragraphs)\n",
        "text = re.sub(r'\\n{2,}', '\\n\\n', text)  # Replace 3+ newlines with 2\n",
        "text = re.sub(r'[ \\t]+', ' ', text)\n",
        "\n",
        "print(text[514:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1OEUDZAk5UV",
        "outputId": "cecbff96-666c-4bd7-9cca-1cc86cfe5ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['\\n', ' ', '!', \"'\", ',', '-', '.', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—']\n",
            "62\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(set(text))\n",
        "print(chars)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGOEyoZ6e-Cu",
        "outputId": "73913715-be3c-42e5-d690-f7c29a321cc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([12, 49, 52, 49, 54, 42, 59,  1, 35, 48, 38,  1, 54, 42, 39,  1, 31, 43,\n",
            "        60, 35, 52, 38,  1, 43, 48,  1, 23, 60,  0,  0,  1,  9,  1, 14, 35, 43,\n",
            "        54, 42, 40, 55, 46,  1, 26, 39, 37, 49, 52, 38,  1, 49])\n"
          ]
        }
      ],
      "source": [
        "string_to_int = {char: i for i, char in enumerate(chars)}\n",
        "int_to_string = {i: char for i, char in enumerate(chars)}\n",
        "\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda l: ''.join(int_to_string[i] for i in l)\n",
        "\n",
        "# Encoding entire text within wizard_of_oz.txt\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data[:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "block_size = 8\n",
        "batch_size = 32\n",
        "\n",
        "n_embed = 256\n",
        "n_layer = 4\n",
        "n_head = 3\n",
        "dropout = 0.2\n",
        "learning_rate = 2e-4 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VERs1zzve-Cv",
        "outputId": "90167846-9ca0-4260-ab68-3db74d5113bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample_size: 194924 48732\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Split 80:20 for train and val\n",
        "n = int(0.8 * len(data))\n",
        "\n",
        "train_data, val_data = data[:n], data[n:]\n",
        "print(\"Sample_size:\", len(train_data), len(val_data))\n",
        "\n",
        "def get_batch(data, batch_size, block_size):\n",
        "    \"\"\" Batch Generator \"\"\"\n",
        "    num_batches = len(data) // batch_size   # Ensure complete batches\n",
        "    indices = list(range(num_batches * batch_size))\n",
        "    random.shuffle(indices)                 # Shuffle for randomness\n",
        "\n",
        "    for i in range(0, len(indices), batch_size):\n",
        "        batch_indices = indices[i:i + batch_size]\n",
        "        batch_indices = [idx % (len(data) - block_size) for idx in batch_indices]\n",
        "\n",
        "        # Generate Input and Target Batch (character level token)\n",
        "        x = torch.stack([data[i:i + block_size] for i in batch_indices])            # Input\n",
        "        y = torch.stack([data[i + 1:i + block_size + 1] for i in batch_indices])    # Targets: Input Shift right by 1\n",
        "\n",
        "        # Push batches to device (preferrebly to CUDA)\n",
        "        yield x.to(device), y.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT Model (Decoder Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Linear Layers follwed by non-linearity \"\"\"\n",
        "    def __init__(self, n_embed, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(dropout),\n",
        "        ) \n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mHead\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Head for Self-Attention (Scaled-Dot Product) \"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, head_size):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" Head for Self-Attention (Scaled-Dot Product) \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Lienar(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Lienar(n_embed, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input: (B, T, C) -> Output: (B, T, head_size)\n",
        "        B, T, C = x.shape                   # Unpack Input Dimensions\n",
        "        k, q = self.key(x), self.query(x)   # (B, T, head_size)\n",
        "\n",
        "        # Compute attn_scores(attn_weights)\n",
        "        w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5             # (B, T, head_size) @ # (B, head_size, T) -> (B, T, T)\n",
        "        w = w.masked_fill(self.trill[:T, :T] == 0, float(\"-inf\"))   # (B, T, T)\n",
        "        w = F.softmax(w, dim=-1)                                    # (B, T, T)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Perform the weighted aggregation of the values\n",
        "        v = self.value(x)                   # (B, T, head_size)\n",
        "        out = w @ v                         # (B, T, T) @ # (B, T, head_size) -> (B, T, head_size)\n",
        "        return out\n",
        "        \n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" Multiple heads of Attn in Parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])     # Create heads in parallel\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (B, T, C) -> Concat feature(last_dim): (B, T, [h0_1, h0_2, h0_3, h0_4, h1_1, h1_2, h1_3, h1_4, h2_1, h2_2, h2_3, h2_4])\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer Blocks \"\"\"\n",
        "    def __init__(self, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head   # Head_size to capture features\n",
        "        self.self_attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.feed_forward = FeedForward(n_embed)\n",
        "        self.lnorm1 = nn.LayerNorm(n_embed)\n",
        "        self.lnorm2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.self_attn(x)\n",
        "        x = self.lnorm1(x+y)\n",
        "        y = self.feed_forward(x)\n",
        "        x = self.lnorm2(x+y)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYREnhkee-Cw"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# GPT Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGPTModel\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, block_size, n_embed, n_head):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "# GPT Model\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, n_embed, n_head):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, n_embed)\n",
        "        self.positional_embeddings = nn.Embedding(block_size, n_embed)\n",
        "\n",
        "        self.decoder_blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        self.final_layer = nn.Linear(n_embed, vocab_size)\n",
        "        self.final_layernorm = nn.LayerNorm(n_embed)\n",
        "\n",
        "        self.apply(self.__init_weights)\n",
        "\n",
        "    \n",
        "    def __init_weights(self, module):\n",
        "        \"\"\" \n",
        "        Initialize proper (gaussian distribution) weights for stable training and convergence\n",
        "        Docs: https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.normal_\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Initializes weights with a normal (Gaussian) distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                # Set the biases to zero\n",
        "                torch.nn.init.zeros(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            # Initializes embeddings weights with a normal (Gaussian) distribution\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "    def forward(self, index, targets = None):\n",
        "        logits = self.token_embeddings(index)\n",
        "\n",
        "        # Index and targets are both (B, T) tokens of integers\n",
        "        token_embed = self.token_embeddings(index)\n",
        "\n",
        "        # torch.arange(T) -> list of indices\n",
        "        pos_embed = self.positional_embeddings(torch.arange(T, device=device))   # (T, C)\n",
        "        x = token_embed + pos_embed     # (B, T, C)\n",
        "        x = self.decoder_blocks(x)      # (B, T, C)\n",
        "        x = self.final_layernorm(x)     # (B, T, C)\n",
        "        logits = self.final_layer(x)    # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # Unpack logits shape to batch, seq_len, class\n",
        "            B, T, C = logits.shape\n",
        "\n",
        "            # Reshape 3D logits -> 2D logits\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            # Compute loss fn\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, _ = self.forward(index)\n",
        "\n",
        "            # Take only last time step\n",
        "            logits = logits[:, -1, :]   # (B, C)\n",
        "\n",
        "            # Apply softmax to get probs\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1)     # (B, 1)\n",
        "\n",
        "            # Append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1)   # (B, T+1)\n",
        "\n",
        "        return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXO0BNpDe-Cx",
        "outputId": "1eb63cf8-83a5-474e-a956-616ef15a2333"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BigramLangaugeModel(\n",
              "  (token_embedding): Embedding(62, 62)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BigramLangaugeModel(vocab_size).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpWh1wp9e-Cx",
        "outputId": "c70b1ddd-fd93-4d26-811a-7617bbc30211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "zXOuTmJ'yrZFlFLupr—'jdbTW,qmvyNEVllanUWCeWUSe?;lphUX;RiNNOn!UXvF?gLevYxQjvzGdX'M-CG!PRnWvd—Skj?HGsLS\n",
            "ejF wMLckfTZCsy, e—AlraTE.v\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=128)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "djjx5BbXe-Cy"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.8)  # Decay LR\n",
        "epochs = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvpjqYKTe-Cy",
        "outputId": "45a4231a-da9a-490c-8cd8-01d9a0dffa6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 4.6930 | Validation Loss: 4.6545\n",
            "Epoch 2/100 | Train Loss: 4.6143 | Validation Loss: 4.5798\n",
            "Epoch 3/100 | Train Loss: 4.5369 | Validation Loss: 4.5066\n",
            "Epoch 4/100 | Train Loss: 4.4610 | Validation Loss: 4.4346\n",
            "Epoch 5/100 | Train Loss: 4.3863 | Validation Loss: 4.3641\n",
            "Epoch 6/100 | Train Loss: 4.3131 | Validation Loss: 4.2949\n",
            "Epoch 7/100 | Train Loss: 4.2412 | Validation Loss: 4.2271\n",
            "Epoch 8/100 | Train Loss: 4.1708 | Validation Loss: 4.1607\n",
            "Epoch 9/100 | Train Loss: 4.1018 | Validation Loss: 4.0958\n",
            "Epoch 10/100 | Train Loss: 4.0342 | Validation Loss: 4.0323\n",
            "Epoch 11/100 | Train Loss: 3.9681 | Validation Loss: 3.9703\n",
            "Epoch 12/100 | Train Loss: 3.9036 | Validation Loss: 3.9098\n",
            "Epoch 13/100 | Train Loss: 3.8405 | Validation Loss: 3.8507\n",
            "Epoch 14/100 | Train Loss: 3.7789 | Validation Loss: 3.7932\n",
            "Epoch 15/100 | Train Loss: 3.7189 | Validation Loss: 3.7373\n",
            "Epoch 16/100 | Train Loss: 3.6604 | Validation Loss: 3.6828\n",
            "Epoch 17/100 | Train Loss: 3.6035 | Validation Loss: 3.6300\n",
            "Epoch 18/100 | Train Loss: 3.5482 | Validation Loss: 3.5787\n",
            "Epoch 19/100 | Train Loss: 3.4945 | Validation Loss: 3.5290\n",
            "Epoch 20/100 | Train Loss: 3.4423 | Validation Loss: 3.4808\n",
            "Epoch 21/100 | Train Loss: 3.3967 | Validation Loss: 3.4434\n",
            "Epoch 22/100 | Train Loss: 3.3572 | Validation Loss: 3.4069\n",
            "Epoch 23/100 | Train Loss: 3.3188 | Validation Loss: 3.3715\n",
            "Epoch 24/100 | Train Loss: 3.2813 | Validation Loss: 3.3370\n",
            "Epoch 25/100 | Train Loss: 3.2449 | Validation Loss: 3.3035\n",
            "Epoch 26/100 | Train Loss: 3.2094 | Validation Loss: 3.2709\n",
            "Epoch 27/100 | Train Loss: 3.1749 | Validation Loss: 3.2394\n",
            "Epoch 28/100 | Train Loss: 3.1414 | Validation Loss: 3.2087\n",
            "Epoch 29/100 | Train Loss: 3.1089 | Validation Loss: 3.1790\n",
            "Epoch 30/100 | Train Loss: 3.0773 | Validation Loss: 3.1502\n",
            "Epoch 31/100 | Train Loss: 3.0466 | Validation Loss: 3.1222\n",
            "Epoch 32/100 | Train Loss: 3.0168 | Validation Loss: 3.0952\n",
            "Epoch 33/100 | Train Loss: 2.9879 | Validation Loss: 3.0690\n",
            "Epoch 34/100 | Train Loss: 2.9599 | Validation Loss: 3.0436\n",
            "Epoch 35/100 | Train Loss: 2.9328 | Validation Loss: 3.0191\n",
            "Epoch 36/100 | Train Loss: 2.9065 | Validation Loss: 2.9954\n",
            "Epoch 37/100 | Train Loss: 2.8811 | Validation Loss: 2.9725\n",
            "Epoch 38/100 | Train Loss: 2.8565 | Validation Loss: 2.9504\n",
            "Epoch 39/100 | Train Loss: 2.8327 | Validation Loss: 2.9291\n",
            "Epoch 40/100 | Train Loss: 2.8098 | Validation Loss: 2.9086\n",
            "Epoch 41/100 | Train Loss: 2.7898 | Validation Loss: 2.8927\n",
            "Epoch 42/100 | Train Loss: 2.7725 | Validation Loss: 2.8773\n",
            "Epoch 43/100 | Train Loss: 2.7557 | Validation Loss: 2.8624\n",
            "Epoch 44/100 | Train Loss: 2.7394 | Validation Loss: 2.8479\n",
            "Epoch 45/100 | Train Loss: 2.7236 | Validation Loss: 2.8339\n",
            "Epoch 46/100 | Train Loss: 2.7083 | Validation Loss: 2.8204\n",
            "Epoch 47/100 | Train Loss: 2.6935 | Validation Loss: 2.8073\n",
            "Epoch 48/100 | Train Loss: 2.6792 | Validation Loss: 2.7948\n",
            "Epoch 49/100 | Train Loss: 2.6653 | Validation Loss: 2.7826\n",
            "Epoch 50/100 | Train Loss: 2.6520 | Validation Loss: 2.7709\n",
            "Epoch 51/100 | Train Loss: 2.6390 | Validation Loss: 2.7596\n",
            "Epoch 52/100 | Train Loss: 2.6266 | Validation Loss: 2.7488\n",
            "Epoch 53/100 | Train Loss: 2.6145 | Validation Loss: 2.7383\n",
            "Epoch 54/100 | Train Loss: 2.6029 | Validation Loss: 2.7283\n",
            "Epoch 55/100 | Train Loss: 2.5918 | Validation Loss: 2.7187\n",
            "Epoch 56/100 | Train Loss: 2.5810 | Validation Loss: 2.7095\n",
            "Epoch 57/100 | Train Loss: 2.5707 | Validation Loss: 2.7006\n",
            "Epoch 58/100 | Train Loss: 2.5607 | Validation Loss: 2.6921\n",
            "Epoch 59/100 | Train Loss: 2.5512 | Validation Loss: 2.6840\n",
            "Epoch 60/100 | Train Loss: 2.5420 | Validation Loss: 2.6763\n",
            "Epoch 61/100 | Train Loss: 2.5340 | Validation Loss: 2.6703\n",
            "Epoch 62/100 | Train Loss: 2.5273 | Validation Loss: 2.6646\n",
            "Epoch 63/100 | Train Loss: 2.5207 | Validation Loss: 2.6591\n",
            "Epoch 64/100 | Train Loss: 2.5144 | Validation Loss: 2.6538\n",
            "Epoch 65/100 | Train Loss: 2.5082 | Validation Loss: 2.6487\n",
            "Epoch 66/100 | Train Loss: 2.5023 | Validation Loss: 2.6438\n",
            "Epoch 67/100 | Train Loss: 2.4966 | Validation Loss: 2.6391\n",
            "Epoch 68/100 | Train Loss: 2.4912 | Validation Loss: 2.6346\n",
            "Epoch 69/100 | Train Loss: 2.4859 | Validation Loss: 2.6303\n",
            "Epoch 70/100 | Train Loss: 2.4808 | Validation Loss: 2.6261\n",
            "Epoch 71/100 | Train Loss: 2.4759 | Validation Loss: 2.6222\n",
            "Epoch 72/100 | Train Loss: 2.4712 | Validation Loss: 2.6184\n",
            "Epoch 73/100 | Train Loss: 2.4666 | Validation Loss: 2.6147\n",
            "Epoch 74/100 | Train Loss: 2.4623 | Validation Loss: 2.6112\n",
            "Epoch 75/100 | Train Loss: 2.4581 | Validation Loss: 2.6079\n",
            "Epoch 76/100 | Train Loss: 2.4541 | Validation Loss: 2.6047\n",
            "Epoch 77/100 | Train Loss: 2.4502 | Validation Loss: 2.6017\n",
            "Epoch 78/100 | Train Loss: 2.4465 | Validation Loss: 2.5987\n",
            "Epoch 79/100 | Train Loss: 2.4429 | Validation Loss: 2.5960\n",
            "Epoch 80/100 | Train Loss: 2.4394 | Validation Loss: 2.5933\n",
            "Epoch 81/100 | Train Loss: 2.4365 | Validation Loss: 2.5912\n",
            "Epoch 82/100 | Train Loss: 2.4339 | Validation Loss: 2.5893\n",
            "Epoch 83/100 | Train Loss: 2.4314 | Validation Loss: 2.5874\n",
            "Epoch 84/100 | Train Loss: 2.4290 | Validation Loss: 2.5855\n",
            "Epoch 85/100 | Train Loss: 2.4267 | Validation Loss: 2.5837\n",
            "Epoch 86/100 | Train Loss: 2.4244 | Validation Loss: 2.5820\n",
            "Epoch 87/100 | Train Loss: 2.4222 | Validation Loss: 2.5804\n",
            "Epoch 88/100 | Train Loss: 2.4201 | Validation Loss: 2.5788\n",
            "Epoch 89/100 | Train Loss: 2.4180 | Validation Loss: 2.5773\n",
            "Epoch 90/100 | Train Loss: 2.4161 | Validation Loss: 2.5758\n",
            "Epoch 91/100 | Train Loss: 2.4141 | Validation Loss: 2.5744\n",
            "Epoch 92/100 | Train Loss: 2.4123 | Validation Loss: 2.5730\n",
            "Epoch 93/100 | Train Loss: 2.4105 | Validation Loss: 2.5717\n",
            "Epoch 94/100 | Train Loss: 2.4087 | Validation Loss: 2.5704\n",
            "Epoch 95/100 | Train Loss: 2.4070 | Validation Loss: 2.5692\n",
            "Epoch 96/100 | Train Loss: 2.4054 | Validation Loss: 2.5680\n",
            "Epoch 97/100 | Train Loss: 2.4038 | Validation Loss: 2.5669\n",
            "Epoch 98/100 | Train Loss: 2.4023 | Validation Loss: 2.5658\n",
            "Epoch 99/100 | Train Loss: 2.4008 | Validation Loss: 2.5647\n",
            "Epoch 100/100 | Train Loss: 2.3993 | Validation Loss: 2.5637\n",
            "Training Complete!\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for x_train, y_train in get_batch(train_data, batch_size, block_size):\n",
        "        logits, loss = model(x_train, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    avg_train_loss = total_train_loss / num_batches\n",
        "\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_val_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_val, y_val in get_batch(val_data, batch_size, block_size):\n",
        "            _, val_loss = model(x_val, y_val)\n",
        "            total_val_loss += val_loss.item()\n",
        "            num_val_batches += 1\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_val_batches\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Training Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyCxSXsYe-Cz",
        "outputId": "48f3b7ea-8ddd-4f50-85ea-74b4d4ccb581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0]], device='cuda:0')\n",
            "\n",
            "Thes fo ssad he, thtond tazm s, \n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(context)\n",
        "\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=32)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCf7UG_re-C0",
        "outputId": "114e74de-b259-4e7f-96ef-21358a9e59b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wizard of oz at beand\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"wizard of oz \"\n",
        "sample_context = torch.tensor(encode(sample_text), dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated_chars = decode(model.generate(sample_context, max_new_tokens=8)[0].tolist())\n",
        "print(generated_chars)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
